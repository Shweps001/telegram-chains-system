#!/usr/bin/env python3
"""
–†–ê–°–®–ò–†–ï–ù–ò–Ø –î–õ–Ø MASTER BOT
–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª: –æ—á–∏—Å—Ç–∫–∞ –∞–∫–∫–∞—É–Ω—Ç–æ–≤, –ø–∞—Ä—Å–µ—Ä, —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
"""

import asyncio
from pathlib import Path
from telethon import TelegramClient
from telethon.tl.types import Channel, Chat
from telethon.tl.functions.channels import LeaveChannelRequest, GetParticipantsRequest
from telethon.tl.types import ChannelParticipantsSearch
import re
import os

# ===================================
# –î–ò–ù–ê–ú–ò–ß–ï–°–ö–û–ï –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –¶–ï–ü–û–ß–ï–ö
# ===================================

def get_all_chains(only_unified=False):
    """
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞—Ö–æ–¥–∏—Ç –≤—Å–µ .session —Ñ–∞–π–ª—ã –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Ü–µ–ø–æ—á–µ–∫
    
    Args:
        only_unified (bool): –ï—Å–ª–∏ True, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ª—å–∫–æ unified –∞–∫–∫–∞—É–Ω—Ç—ã (chain1/2/3)
                            –ï—Å–ª–∏ False, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å–µ—Å—Å–∏–∏
    
    Returns:
        list: [
            {'num': 1, 'name': 'Chain 1', 'session': 'chain1_session.session', 'type': 'unified'},
            {'num': 2, 'name': 'Probiv 1', 'session': 'probiv1_session.session', 'type': 'probiv'},
            ...
        ]
    """
    sessions_path = Path(__file__).parent / 'sessions'
    chains = []
    
    # –ò—â–µ–º –≤—Å–µ .session —Ñ–∞–π–ª—ã
    if sessions_path.exists():
        session_files = sorted(sessions_path.glob('*.session'))
        
        for session_file in session_files:
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Å–µ—Å—Å–∏–∏
            if 'master_bot' in session_file.name or 'anon' in session_file.name:
                continue
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∞–∫–∫–∞—É–Ω—Ç–∞
            account_type = 'unknown'
            display_name = session_file.stem.replace('_session', '')
            
            if 'unified' in session_file.name or ('chain' in session_file.name and not 'probiv' in session_file.name):
                # Unified –∞–∫–∫–∞—É–Ω—Ç—ã (unified_chain1, unified_chain2, unified_chain3)
                account_type = 'unified'
                match = re.search(r'chain(\d+)', session_file.name)
            elif 'probiv' in session_file.name:
                # –ü–∞—Ä—Å–µ—Ä –∞–∫–∫–∞—É–Ω—Ç—ã
                account_type = 'probiv'
                match = re.search(r'probiv(\d+)', session_file.name)
            elif 'filtr' in session_file.name:
                # –ü—Ä–æ–±–∏–≤ –∞–∫–∫–∞—É–Ω—Ç—ã
                account_type = 'filtr'
                match = re.search(r'filtr(\d+)', session_file.name)
            elif 'invocer' in session_file.name:
                # Invocer –∞–∫–∫–∞—É–Ω—Ç—ã
                account_type = 'invocer'
                match = re.search(r'invocer(\d+)', session_file.name)
            elif 'deleter' in session_file.name:
                # Deleter –∞–∫–∫–∞—É–Ω—Ç—ã
                account_type = 'deleter'
                match = re.search(r'deleter(\d+)', session_file.name)
            else:
                # –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø - –ø—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –Ω–æ–º–µ—Ä
                match = re.search(r'(\d+)', session_file.name)
            
            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è: –µ—Å–ª–∏ –Ω—É–∂–Ω—ã —Ç–æ–ª—å–∫–æ unified, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ
            if only_unified and account_type != 'unified':
                continue
            
            chain_num = int(match.group(1)) if match else 0
            
            chains.append({
                'num': chain_num,
                'name': f'{account_type.capitalize()} {chain_num}',
                'session': session_file.name,
                'display_name': display_name,
                'type': account_type
            })
    
    return sorted(chains, key=lambda x: (x['type'] != 'unified', x['num']))


def get_keyword_templates():
    """
    –ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —à–∞–±–ª–æ–Ω–æ–≤ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    
    Returns:
        dict: {
            'base_keywords': [{'file': 'forex.txt', 'name': 'Forex', 'count': 30}, ...],
            'history_keywords': [...],
            'stop_words': [...]
        }
    """
    templates_path = Path(__file__).parent / 'keyword_templates'
    
    result = {
        'base_keywords': [],
        'history_keywords': [],
        'stop_words': []
    }
    
    if not templates_path.exists():
        templates_path.mkdir(exist_ok=True)
        return result
    
    # –°–∫–∞–Ω–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã —à–∞–±–ª–æ–Ω–æ–≤
    for file in sorted(templates_path.glob('*.txt')):
        # –ß–∏—Ç–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤
        try:
            with open(file, 'r', encoding='utf-8') as f:
                words = [line.strip() for line in f if line.strip()]
                count = len(words)
        except:
            count = 0
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –ø–æ –ø—Ä–µ—Ñ–∏–∫—Å—É –∏–º–µ–Ω–∏
        name = file.stem
        
        if name.startswith('base_keywords_'):
            display_name = name.replace('base_keywords_', '').replace('_', ' ').title()
            result['base_keywords'].append({
                'file': file.name,
                'name': display_name,
                'count': count,
                'path': str(file)
            })
        elif name.startswith('history_keywords_'):
            display_name = name.replace('history_keywords_', '').replace('_', ' ').title()
            result['history_keywords'].append({
                'file': file.name,
                'name': display_name,
                'count': count,
                'path': str(file)
            })
        elif name.startswith('stop_words_'):
            display_name = name.replace('stop_words_', '').replace('_', ' ').title()
            result['stop_words'].append({
                'file': file.name,
                'name': display_name,
                'count': count,
                'path': str(file)
            })
    
    return result


def load_template_words(template_path):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–ª–æ–≤–∞ –∏–∑ —Ñ–∞–π–ª–∞ —à–∞–±–ª–æ–Ω–∞
    
    Args:
        template_path: –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É .txt
    
    Returns:
        list: —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤
    """
    try:
        with open(template_path, 'r', encoding='utf-8') as f:
            words = [line.strip().lower() for line in f if line.strip()]
        return words
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —à–∞–±–ª–æ–Ω–∞: {e}")
        return []


def apply_template_to_chain(chain_num, keyword_type, template_path):
    """
    –ü—Ä–∏–º–µ–Ω—è–µ—Ç —à–∞–±–ª–æ–Ω –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∫ unified —Å–∫—Ä–∏–ø—Ç—É —Ü–µ–ø–æ—á–∫–∏
    
    Args:
        chain_num: –Ω–æ–º–µ—Ä —Ü–µ–ø–æ—á–∫–∏
        keyword_type: 'base_title_keywords', 'history_keywords', 'stop_words_in_title'
        template_path: –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É —à–∞–±–ª–æ–Ω–∞
    
    Returns:
        bool: —É—Å–ø–µ—à–Ω–æ –∏–ª–∏ –Ω–µ—Ç
    """
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–ª–æ–≤–∞ –∏–∑ —à–∞–±–ª–æ–Ω–∞
        words = load_template_words(template_path)
        if not words:
            return False
        
        # –ü—É—Ç—å –∫ unified —Å–∫—Ä–∏–ø—Ç—É
        unified_path = Path(__file__).parent / 'unified' / f'unified_chain{chain_num}.py'
        
        if not unified_path.exists():
            print(f"–§–∞–π–ª {unified_path} –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return False
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Å–ª–æ–≤–∞
        return update_keywords_in_file(str(unified_path), keyword_type, words)
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —à–∞–±–ª–æ–Ω–∞: {e}")
        return False

# ===================================
# –û–ß–ò–°–¢–ö–ê –ê–ö–ö–ê–£–ù–¢–û–í –û–¢ –í–°–ï–• –ì–†–£–ü–ü
# ===================================

async def clear_all_groups(client, chain_num, progress_callback=None):
    """
    –£–¥–∞–ª—è–µ—Ç –í–°–ï –≥—Ä—É–ø–ø—ã –∏ —Å—É–ø–µ—Ä–≥—Ä—É–ø–ø—ã –∏–∑ –∞–∫–∫–∞—É–Ω—Ç–∞
    
    Args:
        client: TelegramClient
        chain_num: –Ω–æ–º–µ—Ä chain (1/2/3)
        progress_callback: —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ (async)
    
    Returns:
        dict —Å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
    """
    stats = {
        'left': 0,
        'skipped': 0,
        'errors': 0,
        'total_checked': 0
    }
    
    try:
        if progress_callback:
            await progress_callback("üîç –ü–æ–ª—É—á–∞—é —Å–ø–∏—Å–æ–∫ –¥–∏–∞–ª–æ–≥–æ–≤...")
        
        dialogs = await client.get_dialogs()
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –≥—Ä—É–ø–ø—ã –∏ —Å—É–ø–µ—Ä–≥—Ä—É–ø–ø—ã
        groups = []
        for d in dialogs:
            entity = d.entity
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ –≥—Ä—É–ø–ø–∞ –∏–ª–∏ —Å—É–ø–µ—Ä–≥—Ä—É–ø–ø–∞
            if isinstance(entity, Chat):
                groups.append(entity)
            elif isinstance(entity, Channel) and getattr(entity, 'megagroup', False):
                groups.append(entity)
        
        if not groups:
            if progress_callback:
                await progress_callback("‚ÑπÔ∏è –ê–∫–∫–∞—É–Ω—Ç –Ω–µ —Å–æ—Å—Ç–æ–∏—Ç –≤ –≥—Ä—É–ø–ø–∞—Ö")
            return stats
        
        if progress_callback:
            await progress_callback(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(groups)} –≥—Ä—É–ø–ø. –ù–∞—á–∏–Ω–∞—é –≤—ã—Ö–æ–¥...")
        
        for i, entity in enumerate(groups, 1):
            stats['total_checked'] += 1
            title = getattr(entity, 'title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')
            
            try:
                await client(LeaveChannelRequest(entity))
                stats['left'] += 1
                
                if progress_callback and i % 5 == 0:
                    await progress_callback(
                        f"‚è≥ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {i}/{len(groups)}\n"
                        f"‚úÖ –í—ã—à–µ–ª: {stats['left']}\n"
                        f"‚ùå –û—à–∏–±–æ–∫: {stats['errors']}"
                    )
                
                # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –≤—ã—Ö–æ–¥–∞–º–∏ –¥–ª—è –∑–∞—â–∏—Ç—ã –æ—Ç FloodWait
                await asyncio.sleep(1.5)
                
            except Exception as e:
                stats['errors'] += 1
                error_msg = str(e)
                
                # –ï—Å–ª–∏ FloodWait - –∂–¥—ë–º –±–æ–ª—å—à–µ
                if 'flood' in error_msg.lower():
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è
                    match = re.search(r'(\d+)', error_msg)
                    if match:
                        wait_time = int(match.group(1))
                        if progress_callback:
                            await progress_callback(
                                f"‚è∏Ô∏è FloodWait: –∂–¥—É {wait_time} —Å–µ–∫...\n"
                                f"–ü—Ä–æ–≥—Ä–µ—Å—Å: {i}/{len(groups)}"
                            )
                        await asyncio.sleep(wait_time + 5)
        
        if progress_callback:
            await progress_callback(
                f"‚úÖ **–û–ß–ò–°–¢–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê**\n\n"
                f"–í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {stats['total_checked']}\n"
                f"‚úÖ –í—ã—à–µ–ª –∏–∑ –≥—Ä—É–ø–ø: {stats['left']}\n"
                f"‚ùå –û—à–∏–±–æ–∫: {stats['errors']}"
            )
    
    except Exception as e:
        stats['errors'] += 1
        if progress_callback:
            await progress_callback(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {str(e)}")
    
    return stats


# ===================================
# –ü–ê–†–°–ï–† –£–ß–ê–°–¢–ù–ò–ö–û–í –ì–†–£–ü–ü–´
# ===================================

async def parse_group_members(client, group_link, progress_callback=None):
    """
    –ü–∞—Ä—Å–∏—Ç —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –≥—Ä—É–ø–ø—ã –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏—Ö username
    
    Args:
        client: TelegramClient
        group_link: —Å—Å—ã–ª–∫–∞ –Ω–∞ –≥—Ä—É–ø–ø—É (https://t.me/... –∏–ª–∏ @username)
        progress_callback: —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ (async)
    
    Returns:
        list of usernames
    """
    usernames = []
    
    try:
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Å—Å—ã–ª–∫—É
        if group_link.startswith('https://t.me/'):
            group_link = group_link.replace('https://t.me/', '')
        if group_link.startswith('@'):
            group_link = group_link[1:]
        
        if progress_callback:
            await progress_callback(f"üîç –ü–æ–ª—É—á–∞—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≥—Ä—É–ø–ø–µ...")
        
        # –ü–æ–ª—É—á–∞–µ–º entity –≥—Ä—É–ø–ø—ã
        entity = await client.get_entity(group_link)
        
        if progress_callback:
            await progress_callback(f"üì• –ü–∞—Ä—Å–∏–Ω–≥ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤: {getattr(entity, 'title', group_link)}...")
        
        # –ü–∞—Ä—Å–∏–º —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤
        offset = 0
        limit = 200
        iteration = 0
        
        while True:
            iteration += 1
            
            try:
                participants = await client(GetParticipantsRequest(
                    channel=entity,
                    filter=ChannelParticipantsSearch(''),
                    offset=offset,
                    limit=limit,
                    hash=0
                ))
                
                if not participants.users:
                    break
                
                # –°–æ–±–∏—Ä–∞–µ–º username
                for user in participants.users:
                    if user.username:
                        usernames.append(f"@{user.username}")
                
                offset += len(participants.users)
                
                if progress_callback and iteration % 3 == 0:
                    await progress_callback(
                        f"‚è≥ –°–æ–±—Ä–∞–Ω–æ username: {len(usernames)}\n"
                        f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {offset}"
                    )
                
                # –ï—Å–ª–∏ –ø–æ–ª—É—á–∏–ª–∏ –º–µ–Ω—å—à–µ —á–µ–º limit - —ç—Ç–æ –∫–æ–Ω–µ—Ü
                if len(participants.users) < limit:
                    break
                
                # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                await asyncio.sleep(1)
                
            except Exception as e:
                error_msg = str(e)
                if 'flood' in error_msg.lower():
                    match = re.search(r'(\d+)', error_msg)
                    if match:
                        wait_time = int(match.group(1))
                        if progress_callback:
                            await progress_callback(f"‚è∏Ô∏è FloodWait: –∂–¥—É {wait_time} —Å–µ–∫...")
                        await asyncio.sleep(wait_time + 5)
                        continue
                else:
                    raise
        
        if progress_callback:
            await progress_callback(
                f"‚úÖ **–ü–ê–†–°–ò–ù–ì –ó–ê–í–ï–†–®–ï–ù**\n\n"
                f"–í—Å–µ–≥–æ username: {len(usernames)}"
            )
    
    except Exception as e:
        if progress_callback:
            await progress_callback(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {str(e)}")
        return []
    
    return usernames


# ===================================
# –†–ï–î–ê–ö–¢–ò–†–û–í–ê–ù–ò–ï –ö–õ–Æ–ß–ï–í–´–•/–°–¢–û–ü-–°–õ–û–í
# ===================================

def get_keywords_from_file(filepath, keyword_type='base_title_keywords'):
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ unified —Å–∫—Ä–∏–ø—Ç–∞
    
    Args:
        filepath: –ø—É—Ç—å –∫ unified_chain*.py
        keyword_type: —Ç–∏–ø —Å–ª–æ–≤ (base_title_keywords, history_keywords, stop_words_in_title)
    
    Returns:
        list of keywords
    """
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # –ò—â–µ–º –Ω—É–∂–Ω—ã–π —Å–ø–∏—Å–æ–∫
        pattern = f"{keyword_type}\\s*=\\s*\\[(.*?)\\]"
        match = re.search(pattern, content, re.DOTALL)
        
        if match:
            list_content = match.group(1)
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ —Å—Ç—Ä–æ–∫–∏ –≤ –∫–∞–≤—ã—á–∫–∞—Ö
            keywords = re.findall(r'["\']([^"\']+)["\']', list_content)
            return keywords
        
        return []
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}")
        return []


def update_keywords_in_file(filepath, keyword_type, new_keywords):
    """
    –û–±–Ω–æ–≤–ª—è–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ unified —Å–∫—Ä–∏–ø—Ç–µ
    
    Args:
        filepath: –ø—É—Ç—å –∫ unified_chain*.py
        keyword_type: —Ç–∏–ø —Å–ª–æ–≤
        new_keywords: –Ω–æ–≤—ã–π —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤
    
    Returns:
        bool: —É—Å–ø–µ—Ö –æ–ø–µ—Ä–∞—Ü–∏–∏
    """
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –Ω–æ–≤—ã–π —Å–ø–∏—Å–æ–∫
        formatted_keywords = ',\n    '.join([f'"{kw}"' for kw in new_keywords])
        new_list = f"{keyword_type} = [\n    {formatted_keywords}\n]"
        
        # –ó–∞–º–µ–Ω—è–µ–º —Å—Ç–∞—Ä—ã–π —Å–ø–∏—Å–æ–∫
        pattern = f"{keyword_type}\\s*=\\s*\\[.*?\\]"
        content = re.sub(pattern, new_list, content, flags=re.DOTALL)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)
        
        return True
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –≤ —Ñ–∞–π–ª: {e}")
        return False


def add_keyword(filepath, keyword_type, new_word):
    """–î–æ–±–∞–≤–ª—è–µ—Ç –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ"""
    keywords = get_keywords_from_file(filepath, keyword_type)
    if new_word.lower() not in [k.lower() for k in keywords]:
        keywords.append(new_word.lower())
        return update_keywords_in_file(filepath, keyword_type, keywords)
    return False  # –£–∂–µ –µ—Å—Ç—å


def remove_keyword(filepath, keyword_type, word_to_remove):
    """–£–¥–∞–ª—è–µ—Ç –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ"""
    keywords = get_keywords_from_file(filepath, keyword_type)
    keywords = [k for k in keywords if k.lower() != word_to_remove.lower()]
    return update_keywords_in_file(filepath, keyword_type, keywords)


# ===================================
# –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò
# ===================================

def get_chain_session_info(chain_num):
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–µ—Å—Å–∏–∏ –¥–ª—è unified chain –∞–∫–∫–∞—É–Ω—Ç–∞
    
    Args:
        chain_num (int): –ù–æ–º–µ—Ä —Ü–µ–ø–æ—á–∫–∏ (1, 2, 3)
    
    Returns:
        dict: {api_id, api_hash, session, phone} –∏–ª–∏ None –µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ
    """
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è UNIFIED –∞–∫–∫–∞—É–Ω—Ç–æ–≤ (—Ç–µ, —á—Ç–æ –≤—Å—Ç—É–ø–∞—é—Ç –∏ —Ñ–∏–ª—å—Ç—Ä—É—é—Ç)
    # –î–∞–Ω–Ω—ã–µ –∏–∑ unified_chain1.py, unified_chain2.py, unified_chain3.py
    configs = {
        1: {
            'api_id': 0  # YOUR_API_ID,
            'api_hash': 'YOUR_API_HASH_HERE',
            'session': 'unified_chain1_session.session',  # Unified Chain 1
            'phone': '+1234567890'
        },
        2: {
            'api_id': 0  # YOUR_API_ID,
            'api_hash': 'YOUR_API_HASH_HERE',
            'session': 'unified_chain2_session.session',  # Unified Chain 2
            'phone': '+1234567890'
        },
        3: {
            'api_id': 0  # YOUR_API_ID,
            'api_hash': 'YOUR_API_HASH_HERE',
            'session': 'unified_chain3_session.session',  # Unified Chain 3
            'phone': '+1234567890'
        }
    }
    
    return configs.get(chain_num)


# ===================================
# –ü–†–ï–°–ï–¢–´ - –ì–û–¢–û–í–´–ï –ö–û–ú–ü–õ–ï–ö–¢–´ –°–õ–û–í
# ===================================

def get_all_presets():
    """
    –ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø—Ä–µ—Å–µ—Ç–æ–≤ (–≥–æ—Ç–æ–≤—ã—Ö –∫–æ–º–ø–ª–µ–∫—Ç–æ–≤)
    
    Returns:
        list: [
            {'file': 'preset_1_forex.txt', 'name': 'Forex & Trading', 'icon': 'üíπ'},
            {'file': 'preset_2_crypto.txt', 'name': 'Crypto & Blockchain', 'icon': '‚Çø'},
            ...
        ]
    """
    presets_path = Path(__file__).parent / 'keyword_templates'
    presets = []
    
    # –ò–∫–æ–Ω–∫–∏ –¥–ª—è –ø—Ä–µ—Å–µ—Ç–æ–≤
    preset_icons = {
        'forex': 'üíπ',
        'crypto': '‚Çø',
        'hr': 'üëî',
        'jobs': 'üëî',
        'fintech': 'üí≥',
        'affiliate': 'ü§ù'
    }
    
    if not presets_path.exists():
        return presets
    
    # –ò—â–µ–º —Ñ–∞–π–ª—ã –ø—Ä–µ—Å–µ—Ç–æ–≤
    for file in sorted(presets_path.glob('preset_*.txt')):
        name = file.stem.replace('preset_', '').replace('_', ' ').title()
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏–∫–æ–Ω–∫—É
        icon = 'üì¶'
        for key, emoji in preset_icons.items():
            if key in file.stem.lower():
                icon = emoji
                break
        
        # –ß–∏—Ç–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ —Ñ–∞–π–ª–∞
        try:
            with open(file, 'r', encoding='utf-8') as f:
                first_line = f.readline().strip()
                # –ü–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞ –º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –Ω–∞–∑–≤–∞–Ω–∏–µ: # üì¶ –ö–û–ú–ü–õ–ï–ö–¢ 1: FOREX & TRADING
                if first_line.startswith('#'):
                    name = first_line.split(':', 1)[-1].strip()
        except:
            pass
        
        presets.append({
            'file': file.name,
            'name': name,
            'icon': icon,
            'path': str(file)
        })
    
    return presets


def load_preset(preset_path):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø—Ä–µ—Å–µ—Ç - –≤—Å–µ 3 –≥—Ä—É–ø–ø—ã —Å–ª–æ–≤ (base, history, stop)
    
    Args:
        preset_path: –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –ø—Ä–µ—Å–µ—Ç–∞
    
    Returns:
        dict: {
            'base_title_keywords': ['word1', 'word2', ...],
            'history_keywords': ['word1', 'word2', ...],
            'stop_words_in_title': ['word1', 'word2', ...]
        }
    """
    result = {
        'base_title_keywords': [],
        'history_keywords': [],
        'stop_words_in_title': []
    }
    
    try:
        with open(preset_path, 'r', encoding='utf-8') as f:
            current_section = None
            
            for line in f:
                line = line.strip()
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∏ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
                if not line or line.startswith('#'):
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–µ–∫—Ü–∏—é –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É
                    if 'Base Title Keywords' in line:
                        current_section = 'base_title_keywords'
                    elif 'History Keywords' in line:
                        current_section = 'history_keywords'
                    elif 'Stop Words' in line:
                        current_section = 'stop_words_in_title'
                    continue
                
                # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª–æ–≤–æ –≤ —Ç–µ–∫—É—â—É—é —Å–µ–∫—Ü–∏—é
                if current_section:
                    result[current_section].append(line.lower())
        
        return result
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–µ—Å–µ—Ç–∞: {e}")
        return result


def apply_preset_to_chain(chain_num, preset_path):
    """
    –ü—Ä–∏–º–µ–Ω—è–µ—Ç –ø—Ä–µ—Å–µ—Ç –∫–æ –≤—Å–µ–º 3 —Ç–∏–ø–∞–º —Å–ª–æ–≤ –≤ unified —Å–∫—Ä–∏–ø—Ç–µ —Ü–µ–ø–æ—á–∫–∏
    
    Args:
        chain_num: –Ω–æ–º–µ—Ä —Ü–µ–ø–æ—á–∫–∏ (–∏–ª–∏ 'all')
        preset_path: –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –ø—Ä–µ—Å–µ—Ç–∞
    
    Returns:
        dict: {
            'success': True/False,
            'chains_updated': [1, 2, 3],
            'errors': []
        }
    """
    result = {
        'success': True,
        'chains_updated': [],
        'errors': []
    }
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–µ—Å–µ—Ç
        preset_data = load_preset(preset_path)
        
        if not any(preset_data.values()):
            result['success'] = False
            result['errors'].append("–ü—Ä–µ—Å–µ—Ç –ø—É—Å—Ç–æ–π –∏–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å")
            return result
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ü–µ–ø–æ—á–∫–∏ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
        if chain_num == 'all':
            chains = [c['num'] for c in get_all_chains()]
        else:
            chains = [chain_num]
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫ –∫–∞–∂–¥–æ–π —Ü–µ–ø–æ—á–∫–µ
        for num in chains:
            unified_path = Path(__file__).parent / 'unified' / f'unified_chain{num}.py'
            
            if not unified_path.exists():
                result['errors'].append(f"–§–∞–π–ª unified_chain{num}.py –Ω–µ –Ω–∞–π–¥–µ–Ω")
                continue
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –≤—Å–µ 3 —Ç–∏–ø–∞ —Å–ª–æ–≤
            success_count = 0
            for keyword_type, words in preset_data.items():
                if words and update_keywords_in_file(str(unified_path), keyword_type, words):
                    success_count += 1
            
            if success_count == 3:
                result['chains_updated'].append(num)
            else:
                result['errors'].append(f"Chain {num}: –æ–±–Ω–æ–≤–ª–µ–Ω–æ —Ç–æ–ª—å–∫–æ {success_count}/3")
        
        if not result['chains_updated']:
            result['success'] = False
        
        return result
        
    except Exception as e:
        result['success'] = False
        result['errors'].append(f"–û—à–∏–±–∫–∞ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –ø—Ä–µ—Å–µ—Ç–∞: {e}")
        return result


def merge_presets(preset_paths):
    """
    –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–µ—Å–µ—Ç–æ–≤ –≤ –æ–¥–∏–Ω
    
    Args:
        preset_paths: —Å–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ —Ñ–∞–π–ª–∞–º –ø—Ä–µ—Å–µ—Ç–æ–≤ ['path1.txt', 'path2.txt', ...]
    
    Returns:
        dict: {
            'base_title_keywords': ['word1', 'word2', ...],  # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –≤—Å–µ—Ö –ø—Ä–µ—Å–µ—Ç–æ–≤
            'history_keywords': ['word1', 'word2', ...],
            'stop_words_in_title': ['word1', 'word2', ...]
        }
    """
    merged = {
        'base_title_keywords': set(),
        'history_keywords': set(),
        'stop_words_in_title': set()
    }
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–∞–∂–¥—ã–π –ø—Ä–µ—Å–µ—Ç –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ–º —Å–ª–æ–≤–∞
    for preset_path in preset_paths:
        preset_data = load_preset(preset_path)
        
        for key in merged.keys():
            merged[key].update(preset_data.get(key, []))
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º set –æ–±—Ä–∞—Ç–Ω–æ –≤ list –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º
    result = {
        key: sorted(list(words))
        for key, words in merged.items()
    }
    
    return result


def apply_multiple_presets_to_chain(chain_num, preset_paths):
    """
    –ü—Ä–∏–º–µ–Ω—è–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–µ—Å–µ—Ç–æ–≤ (–æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –∏—Ö) –∫ unified —Å–∫—Ä–∏–ø—Ç—É —Ü–µ–ø–æ—á–∫–∏
    
    Args:
        chain_num: –Ω–æ–º–µ—Ä —Ü–µ–ø–æ—á–∫–∏ (–∏–ª–∏ 'all')
        preset_paths: —Å–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ —Ñ–∞–π–ª–∞–º –ø—Ä–µ—Å–µ—Ç–æ–≤
    
    Returns:
        dict: {
            'success': True/False,
            'chains_updated': [1, 2, 3],
            'merged_counts': {'base': 150, 'history': 120, 'stop': 500},
            'errors': []
        }
    """
    result = {
        'success': True,
        'chains_updated': [],
        'merged_counts': {},
        'errors': []
    }
    
    try:
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø—Ä–µ—Å–µ—Ç—ã
        merged_data = merge_presets(preset_paths)
        
        if not any(merged_data.values()):
            result['success'] = False
            result['errors'].append("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–µ—Å–µ—Ç—ã")
            return result
        
        # –°—á–∏—Ç–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤
        result['merged_counts'] = {
            'base': len(merged_data['base_title_keywords']),
            'history': len(merged_data['history_keywords']),
            'stop': len(merged_data['stop_words_in_title'])
        }
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ü–µ–ø–æ—á–∫–∏ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
        if chain_num == 'all':
            chains = [c['num'] for c in get_all_chains()]
        else:
            chains = [chain_num]
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫ –∫–∞–∂–¥–æ–π —Ü–µ–ø–æ—á–∫–µ
        for num in chains:
            unified_path = Path(__file__).parent / 'unified' / f'unified_chain{num}.py'
            
            if not unified_path.exists():
                result['errors'].append(f"–§–∞–π–ª unified_chain{num}.py –Ω–µ –Ω–∞–π–¥–µ–Ω")
                continue
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –≤—Å–µ 3 —Ç–∏–ø–∞ —Å–ª–æ–≤
            success_count = 0
            for keyword_type, words in merged_data.items():
                if words and update_keywords_in_file(str(unified_path), keyword_type, words):
                    success_count += 1
            
            if success_count == 3:
                result['chains_updated'].append(num)
            else:
                result['errors'].append(f"Chain {num}: –æ–±–Ω–æ–≤–ª–µ–Ω–æ —Ç–æ–ª—å–∫–æ {success_count}/3")
        
        if not result['chains_updated']:
            result['success'] = False
        
        return result
        
    except Exception as e:
        result['success'] = False
        result['errors'].append(f"–û—à–∏–±–∫–∞ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –ø—Ä–µ—Å–µ—Ç–æ–≤: {e}")
        return result
